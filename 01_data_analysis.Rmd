---
title: "Analyse_des_données_de_la_rade_de_Brest"
output: 
  github_document:
    toc: true
    toc_depth: 2
---

# Méthodes
## Des lectures aux tableaux

1/ quelles sont les influences relatives de la profondeur et de la saison sur la structure des communautes planctoniques de la rade de Brest ?
2/ Quels sont les biomarqueurs de saison (hiver et ete) ?

```{r}
library("dada2")
```

Ce premier code permet d’importer les données de l'étude de la rade de Brest, à partir d’un ensemble de fichiers fastq.Ici, on définit une variable chemin path, afin de pouvoir accéder à ces données.

```{r}
path <- "~/CC2EcoG2/Seqreunies" # MODIFIER le répertoire contenant les fichiers fastq après la décompression
list.files(path)
```

## Filtrer les données

On filtre les séquences de faible qualité, puis on les enlève. On demande ici d'afficher les "moins bons".

```{r}
# Le tri permet de s'assurer que les lectures en avant et en arrière sont dans le même ordre
fnFs <- sort(list.files(path, pattern="_R1.fastq"))
fnRs <- sort(list.files(path, pattern="_R2.fastq"))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "R"), `[`, 1)
# Specify the full path to the fnFs and fnRs
fnFs <- file.path(path, fnFs)
fnRs <- file.path(path, fnRs)
```

On sait que plus on se rapproche de la fin des séquençages, moins bonne sera leur qualité. En effet, on remarque que pour les lectures avant (deux premiers graphes), le score de qualité moyen ne descend jamais en dessous de 30. Au contraire, les graphes incarnant la fin des lectures montrent un score de qualité plus bas (~25). Ce type de chiffre représente la probabilité que ce ne soit pas le bon nucléotide d’appelé. De ce fait, avec un Q30 en début de séquences, il y a une chance sur 1000 que ce soit le cas.

```{r}
library(dada2)
plotQualityProfile(fnFs[1:2])
```

```{r}
plotQualityProfile(fnRs[1:2])
```

On voit bien ici la moins bonne qualité des fins de séquences, que ce soit en été ou en hiver. En effet, les scores de qualités baissent vers la position 240 pour les premières lectures, et plutôt vers la position 200 pour les lectures arrières. En prenant ces informations en compte, on va pouvoir dans un premier temps créer des variables pour les fichiers filtrés, puis appliquer la fonction filterAndTrim.

```{r}
filt_path <- file.path(path, "filtered") # Placez les fichiers filtrés dans le sous-répertoire "filtered"
if(!file_test("-d", filt_path)) dir.create(filt_path)
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
sample.names
```

## Filtrez les lectures en amont et en aval

Cette fonction se base sur des fichiers contenant les lectures coupées ayant passées les filtres. "TrimLeft" permet de retirer les primers afin de ne pas les intégrer aux séquences étudiées. 

```{r}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,200), trimLeft=c(21,21),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE
head(out)
```

# Connaître les taux d'erreur

Ci-dessous, la fonction learnErrors permet d’estimer les taux d’erreurs à partir d’un grand ensemble de données. Ainsi, les résultats ci-après expriment le nombre de bases qui sera finalement utilisé, par rapport au premier ensemble.

```{r}
errF <- learnErrors(filtFs, multithread=TRUE)
```

```{r}
errR <- learnErrors(filtRs, multithread=TRUE)
```

```{r}
plotErrors(errF, nominalQ=TRUE)
```

```{r}
library(ggplot2)
library(dada2)
plotErrors(errF, nominalQ=TRUE)
```

Les figures ci-dessus représentent les estimations des taux d’erreurs en fonction du score de qualité. La ligne rouge incarne la tendance générale du graphique. Ensuite, les points noirs reflètent le taux d’erreurs observées, et la ligne noire le taux d’erreurs ajustées. On peut donc observer ci-dessus la fréquence du taux d’erreur en fonction du score de qualité. Aucune différence significative ne peut être relevée entre errR et errF. En effet, on observe la même tendance : moins il y a d’erreurs, plus le score de qualité augmente, ce qui est en accord avec les résultats attendus.

## Exemple d'inférence

La fonction dada retire les erreurs de séquençage et renvoie la composition déduite des échantillons.

```{r}
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
```

```{r}
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)
```

# Fusions des lectures et élimination des chimères

```{r}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# Inspecter le fichier "merger data.frame" du premier échantillon
head(mergers[[1]])
```

## Construire un tableau séquentiel

```{r}
seqtab <- makeSequenceTable(mergers)
# Récupérer ou définir la dimension d'un objet
dim(seqtab)
```

```{r}
# Contrôler la distribution des longueurs de séquence
table(nchar(getSequences(seqtab)))
```

Ici, il supprime les séquences reproduites en comparant chaque séquence aux autres.

## Supprimer les chimères

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
```

On peut donc dire que les chimères représentent environ 11% des variantes de séquences.

```{r}
sum(seqtab.nochim)/sum(seqtab)
```

Il y a environ 23 pourcents de chimères. 

# "Track reads through the pipeline"

```{r}
library(dada2)
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# Si vous traitez un seul échantillon, supprimez les sapply : remplacez sapply(dadaFs, getN) par getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```

# Attribuer une taxonomie

On va assigner une taxonomie aux données de la rade de Brest à partir de ce qu'on pourra observer de Silva. Cela est rendu possible car l’ARN 16S est un marqueur extrêmement bien classé. Tout d’abord, il nous faut importer les données Silva, qui nous serviront à réaliser l’arbre taxonomique. Vous pourrez retrouver les codes nécessaires dans 00_data_import.

```{r}
taxa <- assignTaxonomy(seqtab.nochim, "~/silva_nr99_v138_train_set.fa.gz", multithread=TRUE)
```

```{r}
taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```

On peut ici remarquer la présence de plusieurs genres différents. On pourra étudier leur répartition en fonction de la profondeur et de la saison dans l'étude phyloseq. 

```{r}
save.image(file = "01_data_analysis_FinalEnv")
```


